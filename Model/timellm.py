# -*- coding: utf-8 -*-
"""TimeLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ej0UYLP7KzSPAF7geRqfCYgyLMRysuj0
"""

!pip install neuralforecast transformers gputil

from google.colab import drive

drive.mount('/content/drive')
# Access your dataset via '/content/drive/My Drive/filtered_data.csv'

import os
import torch
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler , StandardScaler , LabelEncoder , OneHotEncoder
from neuralforecast import NeuralForecast
from neuralforecast.models import TimeLLM

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# %%
data = pd.read_csv('/content/drive/My Drive/filtered_data.csv')


# %%
data[['LCLid','ToU','holiday']] = data[['LCLid','ToU','holiday']].astype('object')


# %%
df = data[['LCLid', 'day','energy_sum','temperature_avg','dewPoint','EnergyClass', 'Acorn_grouped','day_of_week', 'month_of_year','holiday']]

# Rename columns to match NeuralForecast requirements
df = df.rename(columns={'LCLid': 'unique_id', 'day': 'ds', 'energy_sum': 'y'})

# Ensure 'ds' is a datetime object
df['ds'] = pd.to_datetime(df['ds'])

# Drop unnecessary columns if needed
df = df[['unique_id', 'ds', 'y']]

print(df.head())

# Select unique IDs
sampled_ids = df['unique_id'].unique()[:1000]  # Adjust for more/less
df_small = df[df['unique_id'].isin(sampled_ids)].copy()

#print(f"Selected unique IDs: {sampled_ids}")
#print(f"Filtered dataset shape: {df_small.shape}")

# Define prediction horizon and encoder input size
h = 14  # Forecast horizon (14 days)
input_size = 2 * 14  # 12 weeks = 84 days

# Split the data
Y_train_df = df[df['ds'] < (df['ds'].max() - pd.Timedelta(days=h + 14))]  # Training set
Y_val_df = df[(df['ds'] >= (df['ds'].max() - pd.Timedelta(days=h + 14))) &
              (df['ds'] < (df['ds'].max() - pd.Timedelta(days=h)))]  # Validation set
Y_test_df = df[df['ds'] >= (df['ds'].max() - pd.Timedelta(days=h))]  # Test set

print(f"Training Set: {Y_train_df.shape}")
print(f"Validation Set: {Y_val_df.shape}")
print(f"Test Set: {Y_test_df.shape}")

# Prompt for the LLM
prompt_prefix = (
    "The dataset contains daily energy consumption data. "
    "We aim to forecast the next 14 days using the last 168 days of data."
    "There is seasonal trend as colder month the energy consumption is higher and vice versa."
    "On weekend energy consumption is higher"
)

# Define the TimeLLM model
timellm = TimeLLM(
    h=14,
    input_size=168,  # Encoder input size (168 days)
    llm='openai-community/gpt2-small',  # LLM backend
    prompt_prefix=prompt_prefix,
    batch_size=1,
    start_padding_enabled=False,
    windows_batch_size=16,
    d_model = 32,
    d_llm= 768,
    d_ff= 128,
    max_steps=10000,
    val_check_steps=500,
    learning_rate=0.0001,
    optimizer=torch.optim.Adam,
    n_heads=8

)

# Initialize NeuralForecast with daily frequency
nf = NeuralForecast(
    models=[timellm],
    freq='D'  # Daily frequency
)

# Fit the model
nf.fit(df=Y_train_df, val_size=14)  # Use the last 14 days of training data for validation

forecasts = nf.predict(futr_df=Y_test_df)

# Merge on unique_id and ds to align actuals with predictions
aligned = Y_test_df.merge(forecasts, on=['unique_id', 'ds'], how='inner')

# Extract aligned actual and predicted values
y_true_aligned = aligned['y'].values  # Aligned actual values
y_pred_aligned = aligned['TimeLLM'].values  # Aligned predicted values

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# MAE
mae = mean_absolute_error(y_true_aligned, y_pred_aligned)

# RMSE
rmse = mean_squared_error(y_true_aligned, y_pred_aligned)

# MAPE
mape = np.mean(np.abs((y_true_aligned - y_pred_aligned) / y_true_aligned)) * 100

# SMAPE
smape = np.mean(2 * np.abs(y_true_aligned - y_pred_aligned) / (np.abs(y_true_aligned) + np.abs(y_pred_aligned))) * 100

# Print results
print(f"MAE: {mae}")
print(f"RMSE: {rmse}")
print(f"MAPE: {mape:.2f}%")
print(f"SMAPE: {smape:.2f}%")

# Get unique series IDs
unique_ids = aligned['unique_id'].unique()

# Plot each series
for unique_id in unique_ids[:10]:  # Limit to the first 10 series for simplicity
    series = aligned[aligned['unique_id'] == unique_id].iloc[:185]  # First 185 points for this ID

    plt.figure(figsize=(10, 5))
    plt.plot(series['ds'], series['y'], label="Actual", color='black')
    plt.plot(series['ds'], series['TimeLLM'], label="TimeLLM Forecast", color='blue', linestyle='dashed')

    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.title(f"TimeLLM Forecast vs. Actual (Series: {unique_id}, First 185 Points)")
    plt.legend()
    plt.grid()
    plt.show()